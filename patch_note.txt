
********************************************
===== V-0.9.3.0 =====
 (interface V-0.6.3)

====    Interface related    ====

- Various minor updates to wiki and readme
- Correct an error in the display of the running Mean Prob. Max IoU
- Update various console outputs to increase density and generate fewer lines overall
- Add a more complete YOLO parameter setting summary-display
- Add information about the total number of weights and the corresponding total "network" memory
  in the perf eval display
  

==== Low level modifications ====

- Fix an error that was preventing the use of a flat input since the last dropout update
- For CUDA compute, all layers now initialize a seed for random number generation (previously only layers with dropout)
- Add more (and better) default values to the YOLO parameters. More optional keywords in set_yolo_params.
- Change the default bias value for YOLO layer to 0.5 
  (better for parameter regression with LIN activation, no apparent negative impact on other parameters)
- Refine the probability derivative error compute to avoid impossible targets with low bit count datatypes.
- Correct a definition error in the DIoU (now properly uses squared distances). 
  It is unclear if it would be better for cases like SDCs => support for both types of DIoU might come at some point.
- Switch back to objectness being (P x IoU). The sole IoU value was too often not informative. 
  Still, keep the Probability-only output with specific fined-grained settings.
- Move almost all YOLO default setting to the low level C init functions (rather than being in the Python interface).

-   *********************    COMPLETE REWORK OF THE ASSOCIATION PROCESS DURING YOLO TRAINING    ********************* 
	+ Associations are now made in the order of the best IoU for all targets in the same grid element.
	+ Include a new "starting phase" during which the association is random (allow all boxes to get in the proper range).
	+ "Good but not best" now uses all targets (even not associated with the current grid element).
		If no good enough prediction is found for a given target (new threshold), it gets associated with the best
		"available" theoretical Prior instead of the closest prediction.*
	+ In addition, there is a reinforced association for the smallest theoretical Prior to overcome the fact that it is 
		less probable to have a random overlap between two small boxes. This takes the form of a scaled surface/volume of the 
		smallest Prior, under which all boxes are associated with the smallest Prior (default value 1.5).
	+ It is also possible to define a random proportion (new user define) of predictions that get associated to the best 
		theoretical Prior regardless of their prediction quality, the default is 5%. This allows the network to regularly
		re-evaluate the use of leftover Priors.
	+ The new "strict box size association" is now independent of Prior sizes ordering in the parameters.
		It now defines a distance proximity to each theoretical Prior. Then define a range of "proper" Priors,
		that can be associated or not, this new number is user defined 
		(the default value is 2 best theoretical Priors that can be used as the best match).
		Repetition of the same box Prior count for only one in the number of best Priors authorized, but they all remain open.
	+ If all the "good" Priors are used, the target will fall in the case of "no good enough prediction found"*
		and will search for the best Prior outside the best theoretical Prior range. This is useful to force all the Priors to
		represent an object in case of a very crowded region in the image.
	+ Finally, note that with this definition, the only way for a target to not be associated with any prediction box
		is when there are more targets than Priors in a grid cell.
  Most fine-grained associations only apply when computing the YOLO layer error propagation. 
  The displayed error remains based on the best IoU association only, so it remains representative of the evolution 
  of the network final performance.

- Add diversity to the "fit_param" behavior (per output part):
	+ fit_param = 1 : compute the output loss using the target
	+ fit_param = 0 : still compute the loss, but with "average/neutral" value corresponding to each output part
		(Pos = 0.5, Size = 0 (==prior), Prob = 0.5, Obj = 0.5, class = 0.5, param = 0.5)
	+ fit_param = 0 : the loss is set to 0 for the corresponding output part
- Add a fit_dim parameter that prevent computing a loss for unused dimensions,
  also set corresponding outputs to the appropriate "neutral" value.  
- Rewrite some parts of the deriv_error and error YOLO functions to improve readability.
- Port all the previous changes in the YOLO layer to the CPU version.



********************************************
===== V-0.9.2.9 =====
 (interface V-0.6.2)

/!\ Break compatibility with previous save format and interface /!\

====    Interface related    ====

- New bias keyword for CONV and DENSE layers (read low-level mod. for details).
- Some keywords for activation function have been changed (LINEAR => LIN, SOFTMAX => SMAX, LOGISTIC => LOGI).
- A char string is now used to pass additional arguments to activation functions
  (e.g leaking factor and saturation for RELU can be set using "RELU_S200.0_L0.1" as activation keyword).
- Add a list of interface functions as an alternative way to construct the properly formatted activation strings
  (e.g cnn.relu(saturation=100.0, leaking=0.2), cnn.logistic(beta=2.0), cnn.softmax(), etc).
- Simplify several interface function names (when non-ambiguous).
- Fix dynamic load for a bug in GIL threading and update the new interface.

==== Low level modifications ====

- Activation function sub_parameters can now be customized on a layer-per-layer basis 
(e.g., beta for Logistic or saturation and leaking factor for RELU).
- Unification of the bias behavior (more aligned with other widely adopted frameworks) => 
  now every layer bias value defines the bias added to its own input matrix. 
  This was already the case for conv layers but not for dense layers. Following the bias propagation through a pivot method,
  a dense layer now updates the bias of the previous dense layer weight matrix (instead of its own). 
  In all cases, the "in_bias" parameter in init_network function remains mandatory and will ALWAYS OVERWRITE any bias 
  value specified locally for the first layer. Saves, load, and interface functions have been updated accordingly.
- bias_value and dropout are now members of the default layer structure (and no more related to the layer type structure).
- Correction of an error (segfault) that was occurring when forwarding the test dataset using 
compute_method = BLAS or NAIV while CIANNA was also compiled with the CUDA option.
- Fix an error for when shuffle every was set to 0 (now only shuffle if shuffle_every > 0 and do nothing otherwise).
- Add an error info display and a proper program termination if the error.txt external monitoring file cannot be opened.



********************************************
===== V-0.9.2.8 =====
 (interface V-0.6.1)
 
/!\ Break compatibility with previous save format and interface /!\

====    Interface related    ====

- Update the Python interface for network and layer creation with more default values and automated 1D->2D->3D completion for the C function call.
- Remove (comment) the formatted I/O-related functions from the interface (depreciated). 
The use of dynamic data loading is now recommended for large datasets by using the recent behavior of "create" and "swap" dataset functions (will add an example script asap).
- Related to the previous point: all datasets created from the python interface must NOT include a bias value (bias parameter depreciated). The bias addition is managed automatically.
- A new example file ex_script_dyn_load.py is provided to illustrate how to use the dynamic data loading functions. The wiki page will be extended to provide more in-depth details.
- Unified saving parameter naming in code, the keyword is now "save_every" and can be <= 0 value to indicate that the network should not be saved automatically.
- The prior existence of "fwd_res" and "net_save" directories is now checked at execution time. They are now created if missing, which prevents several crashes or error stop. 
Consequently, they are no more present in the default git repo structure. 
- Add a CIANNA logo display in ascii art. It can be turned off by setting the no-logo parameter to 1.
- Fix spacing in some terminal displays, and remove unnecessary logs in the default output.

==== Low level modifications ====

- Add internal padding user control. Allows to create increasing size convolutional layers
  (int_padding is a required parameter for layer create for now, no default).
- Add experimental support (CUDA Only) for GAN construction and training. Classical GAN works partially (call for Batch Norm addition to CIANNA).
     Add some experimental functions in the perspective of semi-supervised GAN implementation. (not accessible through the interface for now).
- Add the option to save and load networks in a binary format (forward results are already possible in binary)
- Fix remaining edge error on classical convolution.
- Fix the global behavior of transposed convolution (both for the new user-controlled Tr.Conv and the automated one in backprop).
- Fix a discrepancy between CUDA and CPU implementation in dropout for dense layers (errors in both compute methods might remain for the AVG_MODEL inference).
- Fix an import issue that could occur in a specific environment when compiling the python interface.
- Added interrupt signal handling in the C code when called from the Python interface. This solves the issue with CUDA not freeing up GPU memory when using ctrl-C to abort training. 
(Works to be done on a graceful memory clean in this context).



********************************************
V-0.9.2.7

Starting patch note after version 9.2.7
Before this version, most of the update descriptions were in the commit message.













