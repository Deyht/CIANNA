
===== V-0.9.3.4 =====
 (interface V-0.6.6)
 (continuous update from 9.3.1 to 9.3.4)

/!\ Break compatibility with previous save formats, interface keywords, MC-drop prediction format, and learning rate related values scaling /!\

/!\ Add new Interface API documentation to the Wiki page !!

====    Interface related    ====

- /!\ Move from distutils to setuptools to follow the removal of distutils from python 3.12
- /!\ Change all weights update computations to align with the more classical view of averaged contribution over mini_batch instead of sum.
  This should ease the reproduction of classical architectures and will facilitate the sharing and comparison of the related training hyper-parameters.
  The affected parameters are mainly the learning rate and the weight decay.
- Add a silent option to forward and modify the option on dataset handling to allow true no-output prediction
  (useful for real-time predictions, e.g, camera feed)
- Add two new compact "confmat" display: confmat = 2, with only Precision / Recall and global accuracy displayed
  confmat = 3, with only global accuracy. (Can be useful to reduce output footprint when working with many classes).
- Add a new "adv_size" keyword in init_network that defines the size of the progress bar display.
  (useful to reduce output width footprint for small display or to allow smaller terminal width)
- Add a "nb_layers" keyword in cnn load to get only the first N layers of a saved network (e.g if used on a pre-training task).
  New layers can be added after the ones that have been loaded to fit the new task.
- Add a "class_softmax" keyword in the "set_yolo_params" function (more information in low-level)
- Modify the monitoring output of YOLO => now display the mean IoU for all targets, the mean Probability for all targets, 
  and the proportion of targets that have a prediction above "goog_IoU_lim".
- Relax the input size check when loading a saved network. This helps change input resolution for translational equivariant networks.
- Add a "global" keyword for pool_create following low-level modification (ignore set filter size, but the parameter is still mandatory).
- Add stride and padding keywords for pool_create following pooling rework (see low-level for details).
- Add a weight_decay keyword in the train network function. Change the previous "decay" keyword to "lr_decay" to prevent confusion.
- Add a set_frozen_layers function in both interfaces. It takes as an argument an integer array to specify which layers should be frozen.
  Frozen layers will not update their weights during training. The frozen status where already included in the low-level layer code
  for GAN but was not accessible in the high-level interface before. This status can now be updated manually at any time.
- Add new interface functions that allow constructing the several required configuration arrays for YOLO setup. These functions help
  generate the properly formatted array from a more explicit list of keywords, with better handling of default values if a keyword is
  not specified. e.g "set_error_scales", "fit_parts", "set_IoU_lims", "set_slopes_and_maxes", "set_sm_single".
- Add a new YOLO keyword that specifies the use of the "difficult" flag (see low-level)
- Add a new YOLO keyword that controls the type of error display (see low-level)
- Add new keywords in conv and dense layer creation to control the type of weight initialization and a user scaling on the variance.
- /!\ Add the "strict_size" keyword to the last layer in the example scripts to put the emphasis on the fact that any dense layer without the
  keyword set can be automatically shrink when using Tensor Cores if its dimension is a multiple of 8 (e.g 1000 classes issue). 
- Add an interface to the save network function. This change make renaming and format change of network saves easier.
- Change the performance display to state the layer type.
- Add the "inference_only" keyword in the init function (see low-level for details).
- Add interface functions for the new "norm" layer type (see low-level for details). The keyword for this layer creation function are "group_size" and "set_off".
- Change all reference to a number of "epoch" and the corresponding keywords to iteration (or iter). This change was done to prevent confusion between a real number of epochs
  and the number of paths through a smaller training sub-sample, especially for dynamic augmentation.
- Change conv loading display for elements that are not "volumes" namely stride, padding, and internal padding, for which the "x" splitting is replaced by a ":".
- Add a function to export the network architecture in a .tex formatted table. Also compile it as a .pdf. The table column can be selected individually.
- Add a new YOLO keyword that specifies the type of prior distance association (see low-level).
- Add different categories of "difficult" objects, caracterised by the int value in the difficult flag (see low-level).
- Add a return value to all layer creation functions, return the layer_id integer number. Anticipate more complexe architectures construction.
- Add a "rand_prob" keywork in set_yolo_param (see low-level).

==== Low-level modifications ====

- /!\ Fix an error in YOLO error and deriv that could contaminate association tables between subsequent batches.
  Prediction output was not directly affected, but the association process was corrupted during training, impacting the compute of the error gradient.
  This bug might have impacted the predictive performance of YOLO networks trained on older versions.
- /!\ Change all weights update computations to align with the more classical view of averaged contribution over mini_batch instead of sum.
  This should ease the reproduction of classical architectures and will facilitate the sharing and comparison of the related training hyper-parameters.
  The affected parameters are mainly the learning rate and the weight decay.
- Validated with the latest CUDA 12.0 (no significant performance difference observed atm with the current version)
- Fix a global keyword error that was preventing the proper compilation of non-CUDA compute methods when not compiling the CUDA compute method.
- Small adjustment to default activation limits (better support for FP16).
- Add the possibility not to fit the position in the YOLO loss (in addition to all other fit_parts options).
- Fix an error in the computation of dropout weight scaling for dense layers and ensure consistency between all compute methods
- Add support for Softmax / Cross Entropy activation and loss for YOLO classification. Add a "class_softmax" parameter to
  the YOLO initialization function to "opt-in", the default remains logistic / MSE activation and loss for the moment.
- Add an experimental "joint classification and detection" option for the YOLO layer. Relatively simple at the moment: any target image with nb_box
  set to -1 will consider that the provided box (still has to be defined by the user in the target vector) can only be predicted with an IoU equivalent
  to good_IoU_lim, also update the class and parameters but not positions and sizes.
- Add a "GLOBAL" variant for pooling layers. The filter size is automatically set to the whole spatial dimension of the previous layer.
  Also relax some constraints for pooling layers construction (e.g. now allowed as the first layer). Also moved the type "char" interpretation to the C code.
- Global rework of the pooling layer "class". It is now possible to define independently the pooling size, the stride and to add some padding.
  This allow the use of overlapping pooling operations. (Also fix an error in average pooling behavior).
- Add weight decay in all the versions of weight updates (analog to an L2-norm). The amount of weight decay is specified as a parameter in the train_network function (default 0).
  In practice, the weight decay is scaled by the learning_rate and applied inside of the momentum update.
- /!\ Change the MC-dropout prediction scheme. Previously all batches in the dataset were fully forwarded, and this process was repeated for the 
  specified number of repetitions. Now the first layer that presents dropout is flagged, and each batch is forwarded only once before that layer, and the forward
  repetition is only done after this layer. This helps reduce the total amount of computation and data movements and strongly improves prediction speed for network
  architectures where only the last few layers have dropout. The output layout is changed to follow the new ordering. The disadvantages are that the batch_size used
  for inference has to be known to reshape the output; and that the average loss is now computed as an average over all prediction repetitions 
  (same for other averaged monitor quantities).
- Add a new optional "difficult" flag to the target of object detection with YOLO. This flag allows to specify that some objects are difficult for a variety of reasons 
  (apparent size, occlusion, small apparent fraction, ...). During the training process, objects with this flag will be updated through positive reinforcement only. 
  This behavior is controlled by two new hyper-parameters in "IoU_limits": a diff_IoU_lim, which corresponds to a minimum IoU between prediction and target; 
  and a diff_obj_lim, which corresponds to a minimum predicted objecteness. Difficult objects still act as targets for the "Good but not best" association but 
  can be locked as a complete target only if the predictions respect the two IoU and objectness conditions.
- Change the YOLO error computation for display (/!\ does not affect error gradient computation). It now supports two modes: "Complete": which mimics all non-random aspects
  of the association process used for the gradient computation. "Natural": which ignores most of the association tricks to display a more continuous error evolution during training.
  The idea behind this choice is that the use of several association tricks like: the strict box size association, the low IoU best bow association, the handling of 
  "difficult" objects, all the cascading loss limits, ... ; it can result in an apparent increase in predicted output error because the network is progressively trying to 
  fit more (and more difficult) objects over time. To avoid possible confusion between this type of complex loss behavior and a classical overtraining, the "natural"
  error display mode allows to deactivate all the association tricks for error display, effectively computing error on a fixed number of targets 
  (atm difficult objects do not result in a match for "natural" display but still count for good but not best association).
- Restore diversity in weight initialization functions (Xavier, LeCun, normal and uniform variations) and allow their selection per layer along with a user control variance scaling.
- Fix an error that was preventing the deletion of the "error.txt" file if the error compute interval was larger than 10 epochs.
- Add a size check between output_size and output_dim to prevent user errors in the definition of the last layer (or highlight that the strict_size keyword is mandatory when using TC).
  Also add a condition to the dense layer size alteration to ensure that it can only happen during network creation and not when loading a saved network.
- Fix an error that was occurring when loading a network in text format with the "nb_layer" keyword set. Know "non" layer character between layers are properly ignored.
- Move all the confmat function to CPU side (some functions where still on the GPU even so the batch output was already copied on the host for other operations).
- Fix an error in all convolutional layers activation functions that could corrupt the activation map if the dataset size was not a multiple of the batch_size.
- Add support for "output" activation functions to convolutional layers, so now a conv or pool layer with a spatial size of 1 can be used for classification using
  either logistic or softmax output. All the necessary functions (error, deriv error, confmat, etc) are updated to account for the different data ordering of conv layers.
  This modification is especially useful to construct input-size agnostic networks.
- Add an "inference only" mode that can be set at network initialization to prevent allocation of all the memory related to the backpropagation. 
  This should allow a significant memory saving when trained networks are deployed (especially on systems on light GPUs).
- /!\ Add a new normalization layer. This layer is added with its own set of files for all compute methods and other layers have been adjusted to interact properly with this new layer.
  For now only a "Group" Normalization type is allowed and the group size in user defined. It means that it is also possible to perform "Layer" normalization and "Instance"
  normalization with the present implementation. The normalization layer support a subset of activation function but default is linear. For now normalization layers can be placed   
  either after a conv or a pool layer. A second user define parameter "set_off" allow to specify a number of "group" that should be forced to produce an identity transformation
  between the input and the output of the layer. Note that dropout at any point in the network before a normalization layer is not recommended to prevent variance shifting, but they 
  can be used in combination with dropout if placed after the last normalization layer.
- Dropout has been changed in various ways. The masks are now computed using a global curand generator and an array allocation function. The drop mask is now also computed
  for each element in the batch (instead of one drop mask shared for all). Some ways of declaring dropout (between specific types of layers, etc) are now detected and forbidden
  with a returned error message. Some of these interdiction might be removed in future version if they are found useful in specific contexts.
- The only remaining part of the framework that uses curand_kernel is the YOLO layer. For this one the blockstate is now initialized per thread and not just per block.
- Change all variable name, structure element, and functions referring to "epoch" in favor of iteration (or iter). This change was done to prevent confusion between a real number
  of epochs and the number of paths through a smaller training sub-sample, especially for dynamic augmentation.
- /!\ Add a second normalization layer type with Local Response Normalization as an experimental feature. This layer is added with its own set of files only for CUDA compute method at the moment.
  An LRN layer parameters are the range in number of neurons (only on the neurons/activation-maps direction for now, no spatial inhibition), the k, alpha and beta parameters as expressed in the LRN
  formula to adapt the response type. By default an LRN layer is not activated (linear) but an activation function can be specified. Default values are in "python_module.c" for now.
- Change in prior size format has been reverted
- New save format for YOLO layer. Now save all the parameters required to perform inference from saved model: nb_box, nb_class, nb_param, fit_dim, class_softmax, and all the size priors.
  A network can now be forwarded without the ancillary script setup. Still, to allow new setup on a saved model, a new "no_override" parameter is present in the YOLO setup function
  so the parameters from the save file are ignored.
- Change the default output of YOLO forward saving to better represent this new resolution agnostic behavior. Instead of the raw grid element relative position and log of 
  prior adjust, the new output is automatically converted to the predicted bounding box corner coordinates. A new raw_output flag has been added to the YOLO setup function 
  to restore the previous output behavior.
- Refinement of the YOLO association process: i) The "distance" between predictions and priors are now replaced by a 0 centered IoU comparison, expect for the smallest prior 
  search and forced association. ii) The forced smallest association is now made before the "best prior" forcing and the second is skipped if the first condition is verified.
  Note: With this formalism it is now possible to fully emulate the YOLOv2 association process by setting strict size association to 1, or GoodIoUlim to maximum.
- Add different way of selecting the closest prior to a current box_prediction. "IoU" uses a 0 centered IoU comparison, "Size" uses an euclidian distance in the size space,
  "Offset" uses an euclidian distance in the size-offset space (following box size prediction formalism).
- Rework all activation functions to fix incorrect behavior that occured with a conv layer as output layer.
- Port existing support of dense to conv expansion capability from CUDA only to all compute methods.
- Port existing support of explicit transposed convolution settings for spatial size increase from CUDA only to all compute methods.
- Rework YOLO kernels (CPU and GPU) to improve readibility and reduce some memory and compute overhead.
- Add different categories of "difficult" objects, caracterised by the int value in the difficult flag. 
  0: not difficult, 1: update all if found, 2: update all except class and parameters if found, 3: only update probability and objectness if found.  
  When using the "Natural" error type, all difficult objects are ignored regardless of their type.
- Change in compute performance measurement to increase accuracy when using C_CUDA compute method.
- Add a return value to all layer creation functions, return the layer_id integer number. Anticipate more complexe architectures construction.
- Fix an error for in cuda-kernel random number generation. This was mainly impacting random startup.
- Fix an error in random and Low-IoU best prior association for all compute methods.
- Fix an error in strict box size association that came from latest kernel rework, any strict_box_size_association higer than 1 was in fact considered as 1.
- Add a random prior association probability parameter to YOLO loss. This allow to force random prior association at a given rate in a similar way to random startup. 
  This can be used either as a regularizer or a way to smooth representation transition between prior since each one will see a larger contextual range at a small rate. 


********************************************
===== V-0.9.3.1 =====
 (interface V-0.6.3)

====    Interface related    ====

- Correct an error in the display of the running Mean Prob. Max IoU.
- Fix a display error for YOLO causing class and param error to be != 0 when nb_class or nb_param = 0.
- Fix the B.perf display for the last batch for cases where data.size % batch_size != 0.
- Add display of previously missing hyper-parameters for YOLO.
- Moved some remaining default hyper-parameters setting from the Python interface to the C backend.
  This allows more consistency and fine control, and also ease the use of the direct C interface if required.

==== Low-level modifications ====

- Tested support for the new Nvidia Ada Lovelace architecture, working great on Ubuntu 20.04 with CUDA 11.8 and driver 520.
- Fix an activation error for ReLU, all comp_meth (no impact on results, but remove possible out-of-bound access).
- Fix an error introduced in V-0.9.3.0 on the transfer of YOLO parameters "all IoU limits" to the GPU memory, 
  which was causing the crash of any network with nb_param > 0.
- Modify the YOLO association process introduced in V-9.3.0 to correct a condition-specific error with CUDA comp_method.
  The error was occurring when the size of nb_box x max_nb_obj_per_image was too large. In the previous version, a vector of this size was allocated 
  in each CUDA thread local memory, which is too small for this in some cases. The present correction reworks the association, so it preserves
  the search for "Good but not best" for all target boxes, but then search for best IoU and prior distances only on the target present in the current grid element.
  Also update the CPU version to preserve consistency between compute methods.
- In addition to the previous point, all dynamically allocated arrays inside YOLO-specific kernels have been moved to global GPU memory.
  This change does not affect the results but strongly improves the compute performance of the YOLO layer and removes all possibility of 
  exceeding the CUDA thread local memory through specific network configuration.
- Instead of using a single definition of DIoU there are now two: "DIoU" to use the direct distance ratio
  and "DIoU2" to use the squared distance ratio (which is the classical definition).
- Small refinement of the association process for cases with multiple identical priors: 
 	+ In the case of "best theoretical prior association": the process now searches for the best theoretical prior, but if it 
 	  corresponds to several identical priors it will select the one for which the prediction has the best IoU with the target 
 	  (still excluding priors already associated to a previous target).
	+ In the case of "smallest prior" association, the smallest prior is now searched (and not assumed to be the first one) and
	  if there are multiple occurrences of this prior, it will select the one for which the prediction has the best IoU with the target.
  Note: All these refinements do not change the fact that the "next target" to process is selected in order of best IoU with available boxes
  (only impacted by "strict box size association").
	+ Also removed some conditions that were granting a bypass of strict box association. 
	  This condition is now absolutely strict (even for random startup), and the constraints it implies should only be leveraged by using more boxes
	  or using a larger range of strict association.  
- Add "max_nb_obj_per_image" as a mandatory parameter for YOLO layer configuration. This allows the allocation of the working spaces required by 
  the previous points, and it is also used in a new verification step on the dimension of "net output size", which corresponds to the target array.
- Fix an error that was not correctly updating the deriv error and error compute in YOLO network if the number of targets in the image was 0.


********************************************
===== V-0.9.3.0 =====
 (interface V-0.6.3)

====    Interface related    ====

- Various minor updates to wiki and readme
- Update various console outputs to increase density and generate fewer lines overall
- Add a more complete YOLO parameter setting summary-display
- Add information about the total number of weights and the corresponding total "network" memory
  in the perf eval display
  

==== Low-level modifications ====

- Fix an error that was preventing the use of a flat input since the last dropout update
- Fix an indexing error in the loss display computation
- For CUDA compute, all layers now initialize a seed for random number generation (previously only layers with dropout)
- Add more (and better) default values to the YOLO parameters. More optional keywords in set_yolo_params.
- Change the default bias value for YOLO layer to 0.5 
  (better for parameter regression with LIN activation, no apparent negative impact on other parameters)
- Refine the Probability derivative error compute to avoid impossible targets with low bit count datatypes.
- Correct a definition error in the DIoU (now properly uses squared distances). 
  It is still being determined if it would be better for cases like SDCs => support for both types of DIoU might come at some point.
- Switch back to objectness being (P x IoU). The sole IoU value was too often not informative. 
  Still, keep the Probability-only output with specific fined-grained settings.
- Move almost all YOLO default setting to the low-level C init functions (rather than being in the Python interface).

-   *********************    COMPLETE REWORK OF THE ASSOCIATION PROCESS DURING YOLO TRAINING    ********************* 
	+ Associations are now made in the order of the best IoU for all targets in the same grid element.
	+ Include a new "starting phase" during which the association is random (allow all boxes to get in the proper range).
	+ "Good but not best" now uses all targets (even not associated with the current grid element).
		If no good enough prediction is found for a given target (new threshold), it gets associated with the best
		"available" theoretical Prior instead of the closest prediction.*
	+ In addition, there is a reinforced association for the smallest theoretical Prior to overcome the fact that it is 
		less probable to have a random overlap between two small boxes. This takes the form of a scaled surface/volume of the 
		smallest Prior, under which all boxes are associated with the smallest Prior (default value 1.5).
	+ It is also possible to define a random proportion (new user define) of predictions that get associated to the best 
		theoretical Prior regardless of their prediction quality, the default is 5%. This allows the network to regularly
		re-evaluate the use of leftover Priors.
	+ The new "strict box size association" is now independent of Prior sizes ordering in the parameters.
		It now defines a distance proximity to each theoretical Prior. Then define a range of "proper" Priors,
		that can be associated or not, this new number is user defined 
		(the default value is 2 best theoretical Priors that can be used as the best match).
		Repetition of the same box Prior count for only one in the number of best Priors authorized, but they all remain open.
	+ If all the "good" Priors are used, the target will fall in the case of "no good enough prediction found".
		and will search for the best Prior outside the best theoretical Prior range. This is useful to force all the Priors to
		represent an object in case of a very crowded region in the image.~~ (Removed in 9.3.1)
	+ Finally, note that with this definition, the only way for a target not to be associated with any prediction box
		is when there are more targets than Priors in a grid cell.~~ (Not true anymore in 9.3.1)
  Most fine-grained associations only apply when computing the YOLO layer error propagation. 
  The displayed error remains based on the best IoU association only, so it remains representative of the network's final performance evolution.

- Add diversity to the "fit_param" behavior (per output part):
	+ fit_param = 1 : compute the output loss using the target
	+ fit_param = 0 : still compute the loss, but with "average/neutral" value corresponding to each output part
		(Pos = 0.5, Size = 0 (==prior), Prob = 0.5, Obj = 0.5, class = 0.5, param = 0.5)
	+ fit_param = 0 : the loss is set to 0 for the corresponding output part
- Add a fit_dim parameter that prevent computing a loss for unused dimensions,
  also set corresponding outputs to the appropriate "neutral" value.  
- Rewrite some parts of the deriv_error and error YOLO functions to improve readability.
- Port all the previous changes in the YOLO layer to the CPU version.



********************************************
===== V-0.9.2.9 =====
 (interface V-0.6.2)

/!\ Break compatibility with previous save format and interface /!\

====    Interface related    ====

- New bias keyword for CONV and DENSE layers (read low-level mod. for details).
- Some keywords for activation function have been changed (LINEAR => LIN, SOFTMAX => SMAX, LOGISTIC => LOGI).
- A char string is now used to pass additional arguments to activation functions
  (e.g leaking factor and saturation for RELU can be set using "RELU_S200.0_L0.1" as activation keyword).
- Add a list of interface functions as an alternative way to construct the properly formatted activation strings
  (e.g cnn.relu(saturation=100.0, leaking=0.2), cnn.logistic(beta=2.0), cnn.softmax(), etc).
- Simplify several interface function names (when non-ambiguous).
- Fix dynamic load for a bug in GIL threading and update the new interface.

==== Low-level modifications ====

- Activation function sub_parameters can now be customized on a layer-per-layer basis 
(e.g., beta for Logistic or saturation and leaking factor for RELU).
- Unification of the bias behavior (more aligned with other widely adopted frameworks) => 
  now every layer bias value defines the bias added to its own input matrix. 
  This was already the case for conv layers but not for dense layers. Following the bias propagation through a pivot method,
  a dense layer now updates the bias of the previous dense layer weight matrix (instead of its own). 
  In all cases, the "in_bias" parameter in the init_network function remains mandatory and will ALWAYS OVERWRITE any bias 
  value specified locally for the first layer. Saves, load, and interface functions have been updated accordingly.
- bias_value and dropout are now members of the default layer structure (and no more related to the layer type structure).
- Correction of an error (segfault) that was occurring when forwarding the test dataset using 
compute_method = BLAS or NAIV while CIANNA was also compiled with the CUDA option.
- Fix an error for when shuffle every was set to 0 (now only shuffle if shuffle_every > 0 and do nothing otherwise).
- Add an error info display and a proper program termination if the error.txt external monitoring file cannot be opened.



********************************************
===== V-0.9.2.8 =====
 (interface V-0.6.1)
 
/!\ Break compatibility with previous save format and interface /!\

====    Interface related    ====

- Update the Python interface for network and layer creation with more default values and automated 1D->2D->3D completion for the C function call.
- Remove (comment) the formatted I/O-related functions from the interface (depreciated). 
  The use of dynamic data loading is now recommended for large datasets by using the recent behavior of "create" and "swap" dataset functions (will add an example script asap).
- Related to the previous point: all datasets created from the python interface must NOT include a bias value (bias parameter depreciated). The bias addition is managed automatically.
- A new example file ex_script_dyn_load.py is provided to illustrate how to use the dynamic data loading functions. The wiki page will be extended to provide more in-depth details.
- Unified saving parameter naming in code, the keyword is now "save_every" and can be <= 0 value to indicate that the network should not be saved automatically.
- The prior existence of "fwd_res" and "net_save" directories is now checked at execution time. They are now created if missing, which prevents several crashes or error stop. 
Consequently, they are no more present in the default git repo structure. 
- Add a CIANNA logo display in ascii art. It can be turned off by setting the no-logo parameter to 1.
- Fix spacing in some terminal displays, and remove unnecessary logs in the default output.

==== Low-level modifications ====

- Add internal padding user control. Allows to create convolutional layers that increase the spatial dimensionality
  (int_padding is a required parameter for layer creation at the moment, no default).
- Add experimental support (CUDA Only) for GAN construction and training. Classical GAN works partially (call for Batch Norm addition to CIANNA).
     Add some experimental functions in the perspective of semi-supervised GAN implementation. (not accessible through the interface for now).
- Add the option to save and load networks in a binary format (forward results are already possible in binary)
- Fix remaining edge errors on classical convolution.
- Fix the global behavior of transposed convolution (both for the new user-controlled Tr.Conv and the automated one in backprop).
- Fix a discrepancy between CUDA and CPU implementation in dropout for dense layers (errors in both compute methods might remain for the AVG_MODEL inference).
- Fix an import issue that could occur in a specific environment when compiling the python interface.
- Added interrupt signal handling in the C code when called from the Python interface. This solves the issue with CUDA not freeing up GPU memory when using ctrl-C to abort training. 
(Works to be done on a graceful memory clean in this context).



********************************************
V-0.9.2.7

Starting patch note after version 9.2.7
Before this version, most of the update descriptions were in the commit message.













