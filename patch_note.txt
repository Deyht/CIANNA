
********************************************
===== V-0.9.3.1 =====
 (interface V-0.6.3)

====    Interface related    ====

- Correct an error in the display of the running Mean Prob. Max IoU.
- Fix a display error for YOLO causing class and param error to be != 0 when nb_class or nb_param = 0.
- Fix the B.perf display for the last batch for cases where data.size % batch_size != 0.
- Add display of previously missing hyper-parameters for YOLO.
- Moved some remaining default hyper-parameters setting from the Python interface to the C backend.
  This allow more consistency and fine control, and also ease the use of the direct C interface if required.

==== Low level modifications ====

- Tested support for the new Nvidia Ada Lovelace architecture, working great on Ubuntu 20.04 with CUDA 11.8 and driver 520.
- Fix an activation error for ReLU, all comp_meth (no impact on results, but remove possible out of bound access).
- Fix an error introduced in V-0.9.3.0 on the transfer of YOLO parameters "all IoU limits" to the GPU memory, 
  which was causing the crash of any network with nb_param > 0.
- Modify the YOLO association process introduced in V-9.3.0 to correct a condition-specific error with CUDA comp_method.
  The error was occurring when the size of nb_box x max_nb_obj_per_image was too large. In the previous version, a vector of this size
  was allocated in each CUDA thread local memory, which is too small for this in some cases. The present correction rework the association
  so it preserves the search for "Good but not best" for all target boxes, but then search for best IoU and prior distances only on
  the target present in the current grid element. Also update the CPU version to preserve consistency between compute methods.
- In addition to the previous point, all dynamically allocated arrays inside YOLO-specific kernels have been moved to global GPU memory.
  This change do not affect the results but strongly improve the compute performance of the YOLO layer and remove all possibility of 
  exceeding the CUDA thread local memory through specific network configuration.
- Instead of using a single definition of DIoU there is now two : "DIoU" to use the direct distance ratio
  and "DIoU2" to use the squared distance ratio (which is the classical definition).
- Small refinement of the association process for cases with multiple identical priors: 
 	+ In the case of "best theoretical prior association": the process now search for the best theoretical prior but if it 
 	  corresponds to several identical priors it will select the one for which the prediction has the best IoU with the target 
 	  (still excluding priors already associated to a previous target).
	+ In the case of "smallest prior" association, the smallest prior is now searched (and not assumed to be the first one) and
	  if there is multiple occurrence of this prior it will select the one for which the prediction has the best IoU with the target.
  Note: All these refinement do not change the fact that the "next target" to process is selected in order of best IoU with available boxes
  (only impacted by "strict box size association").
	+ Also removed some conditions that was granting a bypass of strict box association. 
	  This condition is now absolutely strict (even for random startup) and the constrains it implies should 
	  only be leveraged by using more boxes or using a larger range of strict association.  
- Add "max_nb_obj_per_image" as a mandatory parameter for YOLO layer configuration. This allows to allocate the working spaces required by
  the previous points. It will also serve for a new verification step on the dimension of "net output size", which correspond to the target array.
- Fix an error that was not properly updating the deriv error and error compute in YOLO network if the number of target in the image was 0.
 

********************************************
===== V-0.9.3.0 =====
 (interface V-0.6.3)

====    Interface related    ====

- Various minor updates to wiki and readme
- Update various console outputs to increase density and generate fewer lines overall
- Add a more complete YOLO parameter setting summary-display
- Add information about the total number of weights and the corresponding total "network" memory
  in the perf eval display
  

==== Low level modifications ====

- Fix an error that was preventing the use of a flat input since the last dropout update
- Fix an indexing error in the loss display computation
- For CUDA compute, all layers now initialize a seed for random number generation (previously only layers with dropout)
- Add more (and better) default values to the YOLO parameters. More optional keywords in set_yolo_params.
- Change the default bias value for YOLO layer to 0.5 
  (better for parameter regression with LIN activation, no apparent negative impact on other parameters)
- Refine the probability derivative error compute to avoid impossible targets with low bit count datatypes.
- Correct a definition error in the DIoU (now properly uses squared distances). 
  It is unclear if it would be better for cases like SDCs => support for both types of DIoU might come at some point.
- Switch back to objectness being (P x IoU). The sole IoU value was too often not informative. 
  Still, keep the Probability-only output with specific fined-grained settings.
- Move almost all YOLO default setting to the low level C init functions (rather than being in the Python interface).

-   *********************    COMPLETE REWORK OF THE ASSOCIATION PROCESS DURING YOLO TRAINING    ********************* 
	+ Associations are now made in the order of the best IoU for all targets in the same grid element.
	+ Include a new "starting phase" during which the association is random (allow all boxes to get in the proper range).
	+ "Good but not best" now uses all targets (even not associated with the current grid element).
		If no good enough prediction is found for a given target (new threshold), it gets associated with the best
		"available" theoretical Prior instead of the closest prediction.*
	+ In addition, there is a reinforced association for the smallest theoretical Prior to overcome the fact that it is 
		less probable to have a random overlap between two small boxes. This takes the form of a scaled surface/volume of the 
		smallest Prior, under which all boxes are associated with the smallest Prior (default value 1.5).
	+ It is also possible to define a random proportion (new user define) of predictions that get associated to the best 
		theoretical Prior regardless of their prediction quality, the default is 5%. This allows the network to regularly
		re-evaluate the use of leftover Priors.
	+ The new "strict box size association" is now independent of Prior sizes ordering in the parameters.
		It now defines a distance proximity to each theoretical Prior. Then define a range of "proper" Priors,
		that can be associated or not, this new number is user defined 
		(the default value is 2 best theoretical Priors that can be used as the best match).
		Repetition of the same box Prior count for only one in the number of best Priors authorized, but they all remain open.
	+ ~~If all the "good" Priors are used, the target will fall in the case of "no good enough prediction found"
		and will search for the best Prior outside the best theoretical Prior range. This is useful to force all the Priors to
		represent an object in case of a very crowded region in the image.~~ (Removed in 9.3.1)
	+ ~~Finally, note that with this definition, the only way for a target to not be associated with any prediction box
		is when there are more targets than Priors in a grid cell.~~ (Not true anymore in 9.3.1)
  Most fine-grained associations only apply when computing the YOLO layer error propagation. 
  The displayed error remains based on the best IoU association only, so it remains representative of the evolution 
  of the network final performance.

- Add diversity to the "fit_param" behavior (per output part):
	+ fit_param = 1 : compute the output loss using the target
	+ fit_param = 0 : still compute the loss, but with "average/neutral" value corresponding to each output part
		(Pos = 0.5, Size = 0 (==prior), Prob = 0.5, Obj = 0.5, class = 0.5, param = 0.5)
	+ fit_param = 0 : the loss is set to 0 for the corresponding output part
- Add a fit_dim parameter that prevent computing a loss for unused dimensions,
  also set corresponding outputs to the appropriate "neutral" value.  
- Rewrite some parts of the deriv_error and error YOLO functions to improve readability.
- Port all the previous changes in the YOLO layer to the CPU version.



********************************************
===== V-0.9.2.9 =====
 (interface V-0.6.2)

/!\ Break compatibility with previous save format and interface /!\

====    Interface related    ====

- New bias keyword for CONV and DENSE layers (read low-level mod. for details).
- Some keywords for activation function have been changed (LINEAR => LIN, SOFTMAX => SMAX, LOGISTIC => LOGI).
- A char string is now used to pass additional arguments to activation functions
  (e.g leaking factor and saturation for RELU can be set using "RELU_S200.0_L0.1" as activation keyword).
- Add a list of interface functions as an alternative way to construct the properly formatted activation strings
  (e.g cnn.relu(saturation=100.0, leaking=0.2), cnn.logistic(beta=2.0), cnn.softmax(), etc).
- Simplify several interface function names (when non-ambiguous).
- Fix dynamic load for a bug in GIL threading and update the new interface.

==== Low level modifications ====

- Activation function sub_parameters can now be customized on a layer-per-layer basis 
(e.g., beta for Logistic or saturation and leaking factor for RELU).
- Unification of the bias behavior (more aligned with other widely adopted frameworks) => 
  now every layer bias value defines the bias added to its own input matrix. 
  This was already the case for conv layers but not for dense layers. Following the bias propagation through a pivot method,
  a dense layer now updates the bias of the previous dense layer weight matrix (instead of its own). 
  In all cases, the "in_bias" parameter in init_network function remains mandatory and will ALWAYS OVERWRITE any bias 
  value specified locally for the first layer. Saves, load, and interface functions have been updated accordingly.
- bias_value and dropout are now members of the default layer structure (and no more related to the layer type structure).
- Correction of an error (segfault) that was occurring when forwarding the test dataset using 
compute_method = BLAS or NAIV while CIANNA was also compiled with the CUDA option.
- Fix an error for when shuffle every was set to 0 (now only shuffle if shuffle_every > 0 and do nothing otherwise).
- Add an error info display and a proper program termination if the error.txt external monitoring file cannot be opened.



********************************************
===== V-0.9.2.8 =====
 (interface V-0.6.1)
 
/!\ Break compatibility with previous save format and interface /!\

====    Interface related    ====

- Update the Python interface for network and layer creation with more default values and automated 1D->2D->3D completion for the C function call.
- Remove (comment) the formatted I/O-related functions from the interface (depreciated). 
The use of dynamic data loading is now recommended for large datasets by using the recent behavior of "create" and "swap" dataset functions (will add an example script asap).
- Related to the previous point: all datasets created from the python interface must NOT include a bias value (bias parameter depreciated). The bias addition is managed automatically.
- A new example file ex_script_dyn_load.py is provided to illustrate how to use the dynamic data loading functions. The wiki page will be extended to provide more in-depth details.
- Unified saving parameter naming in code, the keyword is now "save_every" and can be <= 0 value to indicate that the network should not be saved automatically.
- The prior existence of "fwd_res" and "net_save" directories is now checked at execution time. They are now created if missing, which prevents several crashes or error stop. 
Consequently, they are no more present in the default git repo structure. 
- Add a CIANNA logo display in ascii art. It can be turned off by setting the no-logo parameter to 1.
- Fix spacing in some terminal displays, and remove unnecessary logs in the default output.

==== Low level modifications ====

- Add internal padding user control. Allows to create increasing size convolutional layers
  (int_padding is a required parameter for layer create for now, no default).
- Add experimental support (CUDA Only) for GAN construction and training. Classical GAN works partially (call for Batch Norm addition to CIANNA).
     Add some experimental functions in the perspective of semi-supervised GAN implementation. (not accessible through the interface for now).
- Add the option to save and load networks in a binary format (forward results are already possible in binary)
- Fix remaining edge error on classical convolution.
- Fix the global behavior of transposed convolution (both for the new user-controlled Tr.Conv and the automated one in backprop).
- Fix a discrepancy between CUDA and CPU implementation in dropout for dense layers (errors in both compute methods might remain for the AVG_MODEL inference).
- Fix an import issue that could occur in a specific environment when compiling the python interface.
- Added interrupt signal handling in the C code when called from the Python interface. This solves the issue with CUDA not freeing up GPU memory when using ctrl-C to abort training. 
(Works to be done on a graceful memory clean in this context).



********************************************
V-0.9.2.7

Starting patch note after version 9.2.7
Before this version, most of the update descriptions were in the commit message.













