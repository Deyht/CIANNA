
********************************************
===== V-0.9.3.0 =====
 (interface V-0.6.2)

====    Interface related    ====

- Various small updates to wiki and readme

==== Low level modifications ====

- Add more (and better) default values to the YOLO parameters. More optional keyword in set_yolo_params.
- Change default bias value for YOLO layer to 0.5 
  (better for paramter regression with LIN activation, no apparent negative impact on other parameters)
- Refine the probability derivative error compute to avoid impossible targets with low bit count datatypes.


********************************************
===== V-0.9.2.9 =====
 (interface V-0.6.2)

/!\ Break compatibility with previous save format and interface /!\

====    Interface related    ====

- New bias keyword for CONV and DENSE layers (read low level mod. for details).
- Some keywords for activation function have been changed (LINEAR => LIN, SOFTMAX => SMAX, LOGISTIC => LOGI).
- Activation function character string is now used to pass additional arguments for activation function
  (e.g leaking factor and saturation for RELU can be set using "RELU_S200.0_L0.1" as activation keyword).
- Add a list of interface functions as an alternative way to construct the properly formatted activation strings
  (e.g cnn.relu(saturation=100.0, leaking=0.2), cnn.logistic(beta=2.0), cnn.softmax(), etc).
- Simplify several interface function names (when non ambiguous).
- Fix dynamic load for a bug in GIL threading and update for new interface.

==== Low level modifications ====

- Activation function sub_parameters can now be customized on a layer per layer basis (e.g beta for Logistic, or saturation and leaking factor for RELU).
- Unification of the bias behavior (more aligned with other widely adopted frameworks) => now every layer bias value defines the bias that is added to its own input matrix. This was already the case for conv layer but not for dense layers. Following the bias propagation through pivot method, a dense layer now update the bias of the previous dense layer weight matrix (instead of its own). In all cases the "in_bias" parameter in init_network function remains mandatory and will ALWAYS OVERWRITE any bias value specified locally for the first layer. Saves, load, and interfaces functions have been updated accordingly.
- bias_value and dropout are now members of the default layer structure (and no more related to the layer type structure).
- Correction of an error (sigfault) that was occurring when forwarding the test dataset using compute_method = BLAS or NAIV while CIANNA was also compiled with the CUDA option.
- Fix an error for when shuffle every was set to 0 (now only shuffle if shuffle_every > 0 and do nothing otherwise).
- Add an error info display and a proper program termination if the error.txt external monitoring file cannot be open.



********************************************
===== V-0.9.2.8 =====
 (interface V-0.6.1)
 
/!\ Break compatibility with previous save format and interface /!\

====    Interface related    ====

- Update the Python interface for network and layer creation with more default values and automated 1D->2D->3D completion for the C function call.
- Remove (comment) the formatted I/O related functions from the interface (depreciated). The use of dynamic data loading is now recommended for large dataset handling using the recent behavior of create and "swap" dataset functions (will add an example script asap).
- Related to previous point: all dataset created from the python interface must NOT include a bias value (bias parameter depreciated), the bias addition is managed automatically.
- A new example file ex_script_dyn_load.py is provided to illustrate how to use the dynamic data loading functions. The wiki page will be extended in order to provide more in depth details.
- Unified saving parameter naming in code, the keyword is now "save_every" and can be <= 0 value to indicate that the network should not be saved automatically.
- The prior existence of "fwd_res" and "net_save" directories is now check at execution time. They are now created if missing, which prevent several crashes or error stop. Consequently they are no more present in the default git repo structure. 
- Add a CIANNA logo display in ascii art. Can be turned off by setting the no-logo parameter to 1.
- Fix spacing in some terminal displays, and remove unnecessary logs in the default output.

==== Low level modifications ====

- Add internal padding user control. Allows to create increasing size convolutional layers
  (int_padding is a required parameter at layer create for now, no default).
- Add experimental support (CUDA Only) for GAN construction and training. Classical GAN works partially (call for Batch Norm addition to CIANNA).
     Add some experimental functions in the perspective of semi-supervised GAN. (not accessible through the interface for now).
- Add option to save and load networks in a binary format (forward results are already possible in binary)
- Fix remaining edge error on classical convolution.
- Fix the global behavior of transposed convolution (both for the new user controlled Tr.Conv and the automated one in backprop).
- Fix a discrepancy between CUDA and CPU implementation in dropout for dense layers (errors in both compute methods might remain for the AVG_MODEL inference).
- Fix an import issue that could occur in specific environment when compiling the python interface.
- Added interrupt signal handling in the C code when called from the Python interface. This solve the issue with CUDA not freeing up GPU memory when using ctrl-C to abort training. (Works to be done on a graceful memory clean in this context).



********************************************
V-0.9.2.7

Starting patch note after version 9.2.7













